This repository is a collection of fine-tuning experiments on various LLMs using the LoRA (Low-Rank Adaptation) technique, which optimizes model training for resource efficiency. Each project targets a unique application of LLMs, from instruction-based interactions to general-purpose conversational agents.

Projects Overview

Project 1: Falcon-7B Instruct with LoRA Fine-Tuning
Description: Fine-tunes the Falcon-7B model for specific instruction-based queries using LoRA and Gradio for interaction.

Project 2: SmolLM2 Prompt Generation
Description: Fine-tunes SmolLM2-1.7B model for prompt-based text generation using OpenWebText.

Project 3: Llama-2-7B Chat Fine-Tuning for Domain-Specific Tasks
Description: Adapts the Llama-2-7B chat model for domain-specific text generation with OpenWebText as a base dataset.

Project 4: FLAN-T5 Instructional Fine-Tuning
Description: Fine-tunes FLAN-T5-XL for instruction-following using Databricks Dolly-15k, optimized for memory efficiency.

Project 5: FLAN-T5 Targeted Instruction Fine-Tuning
Description: This project targets instructional response accuracy for FLAN-T5-XL on Databricks Dolly-15k with LoRA for memory efficiency.
